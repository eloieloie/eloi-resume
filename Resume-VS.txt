Eloi Elimelech
ðŸ“§ me@eloi.in | eloi.elimelech@gmail.com
ðŸ“± +47 92089654 | +91 77027 89897


PROFESSIONAL SUMMARY

	Experienced IT professional with 15+ years of expertise in Data Engineering, Cloud Migration, and Database/Application Support, specializing in delivering modern cloud-native data solutions on GCP and Azure platforms.
	Led cloud-first transformation initiatives migrating legacy data platforms (Oracle, SAS, Business Objects) to modern cloud architectures using BigQuery, Databricks, dbt, and Looker with proven performance improvements
	Architected and deployed real-time event-driven data pipelines using GCP Pub/Sub (messaging service), Eventarc, Airflow, and Cloud Functions for automated data ingestion and processing
	Migrated ML models to Vertex AI, achieving 35% latency reduction and operational cost savings through containerized cloud deployments
	Designed, developed, and automated complex ETL and ELT workflows using Azure Data Factory, Databricks, Airflow, dbt, Python, and PySpark to process and transform large-scale datasets
	Hands-on expertise with modern data stack: BigQuery, Databricks, dbt, Denodo, Airflow, and Informatica PowerCenter
	Implemented GitOps and CI/CD pipelines using GitHub Actions for infrastructure-as-code, automated testing, and continuous deployment workflows
	Strong proficiency in Python, SQL, PL/SQL, Scala, and shell scripting for data pipeline development and automation
	Applied data modeling, optimization, and governance best practices across enterprise-scale data platforms
	Built comprehensive monitoring solutions using Prometheus and Grafana with real-time alerting for operational excellence
	Delivered consistent 24x7 L1/L2/L3 production support across hybrid cloud and on-prem environments, ensuring SLA compliance


CERTIFICATIONS

	Microsoft Certified: Azure AI Engineer Associate (A1-102) â€“ 2026
	Google Cloud Certified â€“ Associate Cloud Engineer â€“ 2025
	Microsoft Certified: Azure Administrator Associate (AZ-104) â€“ 2021
	Microsoft Certified: Azure Fundamentals (AZ-900) â€“ 2021


EDUCATION

	B.Tech in Computer Science, SMEC â€“ JNTU
	Diploma in Computer Science, VMR â€“ SBTET


TECHNICAL SKILLS

	Cloud Platforms:		Google Cloud Platform (BigQuery, IAM, Compute, GCS, Cloud Functions, Firebase, Vertex AI, Pub/Sub, Eventarc), Microsoft Azure (ADF, Blob Storage, Container Registry)
  ETL / BI / Data Tools:		Informatica PowerCenter, Denodo, dbt, Power BI, Looker, Crystal Reports, SSIS, SSRS, Azure Data Factory (ADF)
  Databases:			Teradata, Oracle, SQL Server (2008 R2/2012), T-SQL, Snowflake, Firebase (NoSQL)
	Scheduling Tools:		Control-M, Cisco Tidal
	Scripting:			Unix Shell Scripting, PowerShell
	Languages:			SQL, PL/SQL, Python, Java, Scala, C, C#.NET (Framework 3.5/4.0/4.5), PHP, Perl, JavaScript (jQuery, AJAX), Visual Basic (VB.NET)
	Web & Desktop Technologies:	ASP.NET Web Forms, WinForms, WCF (SOAP/REST), RESTful Web Services, Web Services (ASMX), ADO.NET, LINQ to SQL, Vue.js, Nuxt.js, Node.js
	Operating Systems:		UNIX (Solaris, HP-UX), Windows
	Ticketing & Monitoring:		Remedy, ServiceNow, xMatters, WebEx, Jira, Confluence
	DevOps & Cloud Tools:		Git, GitHub Actions, TFS, SVN, Docker, Kubernetes, Helm, ArgoCD, Grafana, Prometheus, Terraform, Maven, JFrog Artifactory
	Deployment & Packaging:		MSI (Visual Studio Setup Projects), InstallShield, IIS 7.0/7.5, Firebase Hosting


PROFESSIONAL EXPERIENCE

Infosys Ltd â€“ Technology Lead.
April 2018 â€“ Present

Clients:
		- T-Mobile - (2018 - 2022)
    - Sunrise - UPC (2022 - 2023)
    - Telenor (2023 - till date)

	Led cloud-first transformation projects focused on migrating and modernizing legacy data platforms to GCP
	Led cloud migration efforts for large-scale data systems:
		- Oracle to GCP (BigQuery)
		- SAS code/scripts to Dbt
		- Oracle to GCP using Denodo for virtualization
		- Business Objects to Looker reporting platform
		- Re-engineered SAS scripts into BigQuery SQL
		- Used Denodo for interim data virtualization during hybrid cloud setup
		- Migrated legacy ML model from on-premises server to Vertex AI custom container deployment, reducing inference latency by 35% and cutting operational costs
	Applied data modeling, optimization, and governance best practices across migrated data platforms
	Used Denodo to virtualize on-prem and cloud datasets, bridging data access during phased migrations
	Rebuilt enterprise reporting using Looker as part of Business Objects decommissioning initiative
	Designed and developed data pipelines using dbt for transformation and orchestration, integrated with BigQuery
	Developed and deployed data pipelines in Databricks, dbt, and Denodo for ETL workflows and data transformation tasks
	Created data integration pipelines in Azure Data Factory (ADF) for reading and writing data to and from Salesforce API
	Created Airflow orchestration workflows for API data ingestion into BigQuery, eliminating manual processes and ensuring data consistency
	Built real-time event-driven ingestion pipelines using GCP Pub/Sub and Eventarc to automate data loading from Cloud Storage to BigQuery
	Automated data workflows and reporting via Unix Shell scripting, SQL, and GCP Cloud Functions
	Developed and optimized SQL/PLSQL procedures and job orchestration logic
	Worked closely with GCP Architects on provisioning, IAM policies, and resource setup
	Implemented GitOps practices for infrastructure-as-code, application deployment pipelines, and self-service access control
	Built and maintained CI/CD pipelines using GitHub Actions for automated testing, deployment, and continuous integration workflows
	Automated build-to-deploy pipeline using GitHub Actions with Maven to create Fat JARs with bundled dependencies and published artifacts to JFrog Artifactory
	Developed GitHub Actions workflows to automate container image migration from public repositories to Azure Container Registry
	Implemented system and pipeline monitoring using Prometheus and Grafana with real-time alerting for operational visibility
  Experience with Snowflake cloud data warehouse and Azure bucket for integrating data from multiple source systems which include loading nested JSON formatted data into snowflake tables.
	Monitored data loads using Teradata Viewpoint, tuning long-running queries
	Managed Control-M and Informatica job chains across production
	Provided L1/L2/L3 support, handling incidents, root cause analysis, and resolution


NTT DATA â€“ DBA / Application Support Engineer
July 2014 â€“ April 2018

Clients:
		- T-Mobile 

	Provided 24x7 production support across cloud and on-prem data pipelines for Teradata, Oracle, and SQL Server
	Managed Informatica workflows and scheduler jobs in Tidal and Control-M, ensuring SLA compliance based on job criticality
	Collaborated with cross-functional teams to troubleshoot file delays, dependency issues, and job failures
	Managed Teradata environments using Administrator and Viewpoint for monitoring database performance, blocked/idle sessions, and query optimization
	Analyzed load performance and stability using Teradata Viewpoint by comparing against historical run data
	Developed SQL/PL-SQL stored procedures, triggers, packages, and functions for data processing and automation
	Optimized query performance using Teradata Explain plan feature and collaborated with DBAs to tune long-running queries
	Wrote UNIX shell scripts for automated job monitoring, file tracking, and process automation invoking stored procedures and views
	Created custom reports and automated monitoring solutions using shell scripting to track file processing and job statuses
	Automated daily status report generation and scheduled distribution to stakeholders across multiple teams
	Managed database administration tasks including object creation, user/role management, backup/restore operations using TARA and arcmain, and DDL/DML change execution
	Validated and tested new Informatica and Tidal jobs prior to production deployment
	Attended global BI meetings and coordinated activities across Oracle, SQL Server, and Teradata platforms
	Developed and executed test plans, test cases, and regression tests for data quality and functional validation
	Automated test packs and maintained test automation tools for continuous integration
	Delivered 24x7 L1/L2/L3 support ensuring SLA adherence and timely issue resolution in a global, multi-team environment


Datum Info Systems â€“ DBA / Developer
Nov 2010 â€“ July 2014

	Designed and developed full-stack enterprise solutions utilizing C#, WinForms for internal desktop tools, and ASP.NET Web Forms for external web portals
	Architected robust data access layers using ADO.NET and Typed DataSets, optimizing communication between client applications and SQL Server 2008 R2 databases
	Modernized legacy web interfaces by integrating AJAX UpdatePanels and jQuery, significantly improving user experience by reducing full-page postbacks
	Developed and maintained WCF (SOAP) services to centralize business logic, enabling seamless data exchange between desktop and web platforms
	Authored and optimized complex T-SQL stored procedures, triggers, and views to handle high-volume data processing and ensure transactional integrity
	Managed the full software development lifecycle (SDLC) using Visual Studio 2010/2012 and Team Foundation Server (TFS) for version control
	Built and maintained application installers using Visual Studio Setup Projects (MSI) and InstallShield, ensuring proper handling of .NET Framework prerequisites and registry configurations

